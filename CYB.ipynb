{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uht_m-WAyWIm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount (\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1IVnDppzlzs"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA7vau-T0UgO"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('cyberbullying_tweets.csv')  # Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60mcMGem0eK2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnyni6fv0hiS"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWuklTE30q68"
      },
      "outputs": [],
      "source": [
        "!pip install demoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBdwwi6E0uGQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import demoji\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrv5uB3_0y7w"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNCO9XPg1EYP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpeo_GJl03bl"
      },
      "outputs": [],
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "STOPWORDS.update(['rt', 'mkr', 'didn', 'bc', 'n', 'm',\n",
        "                  'im', 'll', 'y', 've', 'u', 'ur', 'don',\n",
        "                  'p', 't', 's', 'aren', 'kp', 'o', 'kat',\n",
        "                  'de', 're', 'amp', 'will', 'wa', 'e', 'like'])\n",
        "stemmer = SnowballStemmer('english')\n",
        "def clean_text(text):\n",
        "\n",
        "    # Remove Hashtag, Mention, URLs\n",
        "    pattern = re.compile(r\"(#[A-Za-z0-9]+|@[A-Za-z0-9]+|https?://\\S+|www\\.\\S+|\\S+\\.[a-z]+|RT @)\")\n",
        "    text = pattern.sub('', text)\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    # Make all text lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Stemming\n",
        "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "    # Removing Punctuations\n",
        "    remove_punc = re.compile(r\"[%s]\" % re.escape(string.punctuation))\n",
        "    text = remove_punc.sub('', text)\n",
        "\n",
        "    # Removing stopwords\n",
        "    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "    # Taking care of emojis\n",
        "    emoji = demoji.findall(text)\n",
        "    for emot in emoji:\n",
        "        text = re.sub(r\"(%s)\" % (emot), \"_\".join(emoji[emot].split()), text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mIWDZ3A08WZ"
      },
      "outputs": [],
      "source": [
        "df['cleaned_text'] = df['tweet_text'].apply(lambda text: clean_text(text))  # Applying the cleaning to the text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDDN9YyW1QBS"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RAEF2UR6gxk"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()  # Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06PRurbM6lmk"
      },
      "outputs": [],
      "source": [
        "df['cleaned_text'].duplicated().sum() # Checking for duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8IfoKQX6oF3"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(\"cleaned_text\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKAuT32m6rBI"
      },
      "outputs": [],
      "source": [
        "df['cleaned_text'].str.isspace().sum()     # Checking for tweets with only whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAXovps06ynr"
      },
      "outputs": [],
      "source": [
        "df = df[df[\"cyberbullying_type\"]!=\"other_cyberbullying\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Exthley62Bm"
      },
      "outputs": [],
      "source": [
        "df['cyberbullying_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V975ZviE6461"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data = df, x = 'cyberbullying_type')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiyYIVo267x2"
      },
      "outputs": [],
      "source": [
        "# Plotting the top 15 words of each cyberbullying type\n",
        "for cyber_type in df.cyberbullying_type.unique():\n",
        "\n",
        "    top50_word = df.cleaned_text[df.cyberbullying_type==cyber_type].str.split(expand=True).stack().value_counts()[:15]\n",
        "\n",
        "    fig = px.bar(top50_word, color=top50_word.values, color_continuous_scale=px.colors.sequential.RdPu, custom_data=[top50_word.values])\n",
        "    fig.update_traces(marker_color='red')\n",
        "    fig.update_traces(hovertemplate='<b>Count: </b>%{customdata[0]}')\n",
        "    fig.update_layout(title=f\"Top 15 words for {cyber_type}\",\n",
        "                     template='simple_white',\n",
        "                     hovermode='x unified')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGtp3TVb7GN8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0Gt1Ly27RnU"
      },
      "outputs": [],
      "source": [
        "X = df['cleaned_text']  # Feature (raw data)\n",
        "y = df['cyberbullying_type']  # Target Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzepNQxG7UfC"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)\n",
        "# Performing the train|test split. This test set is essentially a hold out test set as we'll be performing Cross Validation\n",
        "# using Grid Search which will split our training data into a training and validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5cYFtFF7XJO"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features = 5000)  # Using the TF - IDF Vectorizer to extract top 5000 most important features\n",
        "# from the text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlJ-6btg7cql"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)  # Creating the vocabulary only from the training set to avoid data leakage from\n",
        "X_test_tfidf = tfidf.transform(X_test)        # the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxLc_-L77e8M"
      },
      "outputs": [],
      "source": [
        "X_train_tfidf  # Sparse Matrix is created to save memory since many values are close to 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYbFccoK7jSM"
      },
      "outputs": [],
      "source": [
        "X_test_tfidf  # Sparse Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4DFR2l27l7n"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "tfidf_array_train = X_train_tfidf.toarray()   # Converting the sparse matrix to a numpy array (dense matrix)\n",
        "tfidf_array_test = X_test_tfidf.toarray()     # Converting the sparse matrix to a numpy array (dense matrix)\n",
        "scaled_X_train = scaler.fit_transform(tfidf_array_train)  # Fitting on only training data to avoid data leakage from test data\n",
        "scaled_X_test = scaler.transform(tfidf_array_test) # and then tranforming both training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1ihoZwo7ppZ"
      },
      "outputs": [],
      "source": [
        "# Performing Dimensionality Reduction using Principal Component Analysis\n",
        "from sklearn.decomposition import PCA\n",
        "NUM_COMPONENTS = 5000  # Total number of features\n",
        "pca = PCA(NUM_COMPONENTS)\n",
        "reduced = pca.fit(scaled_X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hW2KuheFr0x"
      },
      "outputs": [],
      "source": [
        "variance_explained = np.cumsum(pca.explained_variance_ratio_)  # Calculating the cumulative explained variance by the components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3zP_wZdQNHK"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "plt.plot(range(NUM_COMPONENTS),variance_explained, color='r')\n",
        "ax.grid(True)\n",
        "plt.xlabel(\"Number of components\")\n",
        "plt.ylabel(\"Cumulative explained variance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ril9uF_20dyn"
      },
      "outputs": [],
      "source": [
        "final_pca = PCA(0.9)\n",
        "reduced_90 = final_pca.fit_transform(scaled_X_train) # Number of Components explaining 90% variance in the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO_qdX3r0d6h"
      },
      "outputs": [],
      "source": [
        "reduced_90_test = final_pca.transform(scaled_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_DfQjHY0d-f"
      },
      "outputs": [],
      "source": [
        "reduced_90.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWPrdEem0eBn"
      },
      "outputs": [],
      "source": [
        "#3999 components explain 90% of the variance in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMPPpM253cYp"
      },
      "outputs": [],
      "source": [
        "final_pca = PCA(0.8)\n",
        "reduced_80 = final_pca.fit_transform(scaled_X_train) # Number of Components explaining 80% variance in the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0jYNw4u3gVG"
      },
      "outputs": [],
      "source": [
        "reduced_80.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnppjzT06yzp"
      },
      "outputs": [],
      "source": [
        "# 3290 components explain 80% of the variance in the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDbskbk768kF"
      },
      "source": [
        "## **Training the Model **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af_tFTEh64uX"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEvCSWRV7Dgt"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.run([\"ls\", \"-l\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW30RSV87lS-"
      },
      "outputs": [],
      "source": [
        "subprocess.call([\"ls\", \"-l\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZe1APAV8rT4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfxCcUNY8CRW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o9YM4q6bKvR"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lpri5Jo8L5F"
      },
      "outputs": [],
      "source": [
        "# LOGISTIC REGRESSION with the the 90% variance data\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_model_pca = LogisticRegression()\n",
        "log_model_pca.fit(reduced_90, y_train)\n",
        "preds_log_model_pca = log_model_pca.predict(reduced_90_test)\n",
        "print(classification_report(y_test, preds_log_model_pca))\n",
        "confusion_matrix(y_test, preds_log_model_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKpc5CRHAQcM"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "preds_log_model_pca = log_model_pca.predict(reduced_90_test)\n",
        "print(classification_report(y_test, preds_log_model_pca))\n",
        "confusion_matrix(y_test, preds_log_model_pca)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn.base"
      ],
      "metadata": {
        "id": "XsxAUJ93Fl9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWyRLx_jpHpI"
      },
      "source": [
        "**SUPPORT VECTOR MACHINES **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSsEIPNxBoxl"
      },
      "outputs": [],
      "source": [
        "from sklearn.experimental import enable_halving_search_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfTDJiKVo7tF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import HalvingGridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3LB_79FFuRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3MZZekoo7io"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUvpvu3FBous"
      },
      "outputs": [],
      "source": [
        "# SUPPORT VECTOR MACHINES\n",
        "from sklearn.svm import LinearSVC\n",
        "svm_model = LinearSVC()\n",
        "C = [1e-5, 1e-4, 1e-2, 1e-1, 1]\n",
        "param_grid = {'C': C}\n",
        "grid_svm_model = HalvingGridSearchCV(svm_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)\n",
        "grid_svm_model.fit(X_train_tfidf, y_train)\n",
        "preds_grid_svm_model = grid_svm_model.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, preds_grid_svm_model))\n",
        "plot_confusion_matrix(grid_svm_model, X_test_tfidf, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2kr9-8vBosB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "grid_svm_model.best_estimator_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGOomYT_eV9Z"
      },
      "outputs": [],
      "source": [
        "# NEURAL NETWORKS\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "nn_model = MLPClassifier(activation = 'logistic', max_iter = 10)  # Sigmoid Activation Function\n",
        "param_grid = {'learning_rate_init': [0.001, 0.0015, 0.002, 0.0025]}\n",
        "grid_nn_model = HalvingGridSearchCV(nn_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)\n",
        "grid_nn_model.fit(X_train_tfidf, y_train)\n",
        "preds_grid_nn_model = grid_nn_model.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, preds_grid_nn_model))\n",
        "plot_confusion_matrix(grid_nn_model, X_test_tfidf, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n_AWyMveWBH"
      },
      "outputs": [],
      "source": [
        "grid_nn_model.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxAKZ_KBeWEM"
      },
      "outputs": [],
      "source": [
        "# RANDOM FORESTS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(random_state = 42)\n",
        "n_estimators = [64, 100, 128]\n",
        "bootstrap = [True, False] # Bootstrapping is true by default\n",
        "param_grid = {'n_estimators': n_estimators, 'bootstrap': bootstrap}\n",
        "grid_rf_model = HalvingGridSearchCV(rf_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)\n",
        "grid_rf_model.fit(X_train_tfidf, y_train)\n",
        "preds_grid_rf_model = grid_rf_model.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, preds_grid_rf_model))\n",
        "plot_confusion_matrix(grid_rf_model, X_test_tfidf, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7G5hR0AeWJY"
      },
      "outputs": [],
      "source": [
        "grid_rf_model.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaZlEMgreWNC"
      },
      "outputs": [],
      "source": [
        "# GRADIENT BOOSTING\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "grad_model = GradientBoostingClassifier(random_state = 42)\n",
        "param_grid = {'n_estimators': [64, 100, 128, 200]}\n",
        "grid_grad_model = HalvingGridSearchCV(grad_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)\n",
        "grid_grad_model.fit(X_train_tfidf, y_train)\n",
        "preds_grid_grad_model = grid_grad_model.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, preds_grid_grad_model))\n",
        "plot_confusion_matrix(grid_grad_model, X_test_tfidf, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQmQyF5XeWQs"
      },
      "outputs": [],
      "source": [
        "grid_grad_model.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfLSk_PaeWUE"
      },
      "outputs": [],
      "source": [
        "# NAIVE - BAYES\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "preds_nb_model = nb_model.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, preds_nb_model))\n",
        "plot_confusion_matrix(nb_model, X_test_tfidf, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fykJhwBae21G"
      },
      "source": [
        "**Making Machine Learning Pipelines**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KanvZSkPeWXt"
      },
      "outputs": [],
      "source": [
        "# Creating a pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipe = Pipeline([('tfidf', TfidfVectorizer(max_features = 5000)), ('rf_model', RandomForestClassifier(n_estimators = 128, random_state = 42))])\n",
        "pipe.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhNCnL4teWe_"
      },
      "outputs": [],
      "source": [
        "pipe.predict([\"@abc Hey man! Great match today. Your smashes were spot on. Let's continue playing together. \\\n",
        "               #badminton #brotherhood #men #doubles\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCkrC85YfMvW"
      },
      "source": [
        "**SAMPLE PREDICTIONS **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJI_YT0HfEia"
      },
      "outputs": [],
      "source": [
        "pipe.predict([\"Going to Africa. Hope I don't get AIDS. Just kidding. I'm white!\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyDnGNNFfEmi"
      },
      "outputs": [],
      "source": [
        "pipe.predict([\"Muslims should be punished. We are not doing enough to rid us of those filthy animals.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygzX9AbXfVDX"
      },
      "outputs": [],
      "source": [
        "pipe.predict([\"@abc Man you don't have any facial hair. You look like a fucking 9 year old school boy! #clown #idiot\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx013_GzfU6P"
      },
      "outputs": [],
      "source": [
        "pipe.predict([\"@abc shut up gay boy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X_uyCzDfEpI"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "models = [log_model_pca]\n",
        "#models = [log_model_pca, grid_svm_model, grid_nn_model, grid_rf_model, grid_grad_model, nb_model, pipe]\n",
        "for model in models:\n",
        "    filename = model.__class__.__name__ + '.pkl'\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdagb6OLAMjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SRWWBNBAMYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RhbvfqgIAMVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i2ZsFPTIAMSg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}